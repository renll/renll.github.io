<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Liliang Ren</title> <meta name="author" content="Liliang Ren"/> <meta name="description" content=""/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://renll.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Bio<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Liliang Ren </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="social"> <div class="contact-icons"> <a href="mailto:%72%65%6E%6C%6C%31%34%30%32@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=9MBMglQAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/renll" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/liliang-ren-ba0529181" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/liliang_ren" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </div> <div class="clearfix"> <p>Hi! I am a Member of Technical Staff at Microsoft CoreAI, and a PhD graduate in <a href="https://cs.illinois.edu/" target="_blank" rel="noopener noreferrer">Computer Science</a> from <a href="https://illinois.edu/" target="_blank" rel="noopener noreferrer">University of Illinois Urbana-Champaign</a>, where I am fortunate to be advised by Prof. <a href="http://czhai.cs.illinois.edu/" target="_blank" rel="noopener noreferrer">Chengxiang Zhai</a>. I obtained my master degree at <a href="https://jacobsschool.ucsd.edu/" target="_blank" rel="noopener noreferrer">University of California San Diego</a> advised by Prof. <a href="https://cseweb.ucsd.edu/~jmcauley/" target="_blank" rel="noopener noreferrer">Julian McAuley</a>, and my undergraduate degree at <a href="https://www.ji.sjtu.edu.cn/" target="_blank" rel="noopener noreferrer">Shanghai Jiao Tong University</a> working with Prof. <a href="https://x-lance.github.io/kaiyu/" target="_blank" rel="noopener noreferrer">Kai Yu</a>.</p> <p>My long-term research goal is to develop artificial neural architectures with superhuman efficiency and capacity, and to apply them toward discovering new structured knowledge from natural signals. I believe in two principles:</p> <ul> <li>‚ÄúEverything should be made as simple as possible, but not simpler.‚Äù - Albert Einstein</li> <li>‚ÄúDon‚Äôt give up on the intuition until you figured out why it‚Äôs wrong.‚Äù - Geoffrey Hinton</li> </ul> <p>Recently, I am interested in <strong>large-scale pretraining</strong> of frontier OpenAI models.</p> <p>For a complete list of publications, please check <a href="https://scholar.google.com/citations?user=9MBMglQAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">here</a>.</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep 18, 2025</th> <td> Got 5 papers accepted by NeurIPS 2025, including <a href="https://arxiv.org/abs/2507.06607" target="_blank" rel="noopener noreferrer">SambaY</a>, <a href="https://arxiv.org/abs/2506.18145" target="_blank" rel="noopener noreferrer">Routing Mamba</a>, <a href="https://arxiv.org/abs/2505.16381" target="_blank" rel="noopener noreferrer">PaTH Attention</a>, <a href="https://arxiv.org/abs/2504.20571" target="_blank" rel="noopener noreferrer">1-shot RLVR</a> and <a href="https://arxiv.org/abs/2507.07694" target="_blank" rel="noopener noreferrer">SAS</a>! </td> </tr> <tr> <th scope="row">Jul 9, 2025</th> <td> Super excited to release <a href="https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning" target="_blank" rel="noopener noreferrer">Phi-4-mini-flash-reasoning</a> with our <a href="https://arxiv.org/abs/2507.06607" target="_blank" rel="noopener noreferrer">paper</a> and training <a href="https://github.com/microsoft/ArchScale" target="_blank" rel="noopener noreferrer">codebase</a> on ŒºP++ and the SambaY architecture! </td> </tr> <tr> <th scope="row">Aug 7, 2024</th> <td> Glad to give <a href="https://docs.google.com/presentation/d/1SUGKM_sTKgb-wXSYvalWtD2O2qTKnYuMzid669GfJbU/edit?usp=sharing" target="_blank" rel="noopener noreferrer">talks</a> on Samba at <a href="https://cohere.com/events/cohere-for-ai-liliang-ren" target="_blank" rel="noopener noreferrer">Cohere For AI</a> and <a href="https://icebergnlp.github.io/" target="_blank" rel="noopener noreferrer">BergLab</a> at University of California San Diego. </td> </tr> <tr> <th scope="row">Jun 12, 2024</th> <td> Excited to announce our Samba-3.8B <a href="https://arxiv.org/abs/2406.07522" target="_blank" rel="noopener noreferrer">paper</a>! It outperforms Phi-3-mini on major benchmarks by a large margin, while having infinite context length with linear complexity!üöÄ Checkout my <a href="https://x.com/liliang_ren/status/1801027052147216457" target="_blank" rel="noopener noreferrer">twitter</a> for a brief introduction! </td> </tr> <tr> <th scope="row">Sep 21, 2023</th> <td> One <a href="https://arxiv.org/abs/2306.11197" target="_blank" rel="noopener noreferrer">paper</a> on Efficient Sequence Modeling got accepted by NeurIPS 2023! </td> </tr> <tr> <th scope="row">Jul 31, 2023</th> <td> Gave a <a href="https://drive.google.com/file/d/1HrBSA5fnw8olx2pbNVJ8JVw3Hxh4fQKE/view?usp=sharing" target="_blank" rel="noopener noreferrer">talk</a> on my recent work of Sparse Modular Activation at Microsoft Research! </td> </tr> <tr> <th scope="row">Jul 18, 2023</th> <td> Proud to announce the <a href="https://arxiv.org/abs/2306.11197" target="_blank" rel="noopener noreferrer">paper</a> and the <a href="https://github.com/renll/SeqBoat" target="_blank" rel="noopener noreferrer">code</a> of my previous internship work on Efficient Sequence Modeling at Microsoft Research! </td> </tr> <tr> <th scope="row">May 23, 2023</th> <td> One <a href="https://arxiv.org/abs/2306.15245" target="_blank" rel="noopener noreferrer">paper</a> on Dialogue Evaluation is accepted for oral presentation at the DialDoc workshop at ACL 2023! </td> </tr> <tr> <th scope="row">May 15, 2023</th> <td> Started my summer research internship at <a href="https://www.amazon.science/" target="_blank" rel="noopener noreferrer">Amazon Science</a>, working on a confidential project of Large Language Model! </td> </tr> <tr> <th scope="row">Oct 6, 2022</th> <td> One <a href="https://arxiv.org/abs/2210.12582" target="_blank" rel="noopener noreferrer">paper</a> on Language Model Pre-training got accepted as <strong>Oral</strong> presentation by EMNLP 2022! </td> </tr> <tr> <th scope="row">May 15, 2022</th> <td> Started my summer Research Intern at the <a href="https://www.microsoft.com/en-us/research/group/knowledge-and-language/" target="_blank" rel="noopener noreferrer">Knowledge and Language Team</a> of <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="noopener noreferrer">Microsoft Research</a>! Working with <a href="https://www.microsoft.com/en-us/research/people/yaliu10/" target="_blank" rel="noopener noreferrer">Yang Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/shuowa/" target="_blank" rel="noopener noreferrer">Shuohang Wang</a> and <a href="https://www.microsoft.com/en-us/research/people/yicxu/" target="_blank" rel="noopener noreferrer">Yichong Xu</a>. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="ren2025decoder" class="col-sm-8"> <div class="title">Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation</div> <div class="author"> <em>Liliang Ren</em>,¬†Congcong Chen,¬†Haoran Xu,¬†Young Jin Kim,¬†Adam Atkinson,¬†Zheng Zhan,¬†Jiankai Sun,¬†Baolin Peng,¬†Liyuan Liu,¬†Shuohang Wang,¬†Hao Cheng,¬†Jianfeng Gao,¬†Weizhu Chen,¬†and¬†Yelong Shen</div> <div class="periodical"> <em>Proceedings of the 39th Conference on Neural Information Processing Systems,</em> Dec 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/microsoft/ArchScale" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://arxiv.org/abs/2507.06607" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">author = {Ren, Liliang and Chen, Congcong and Xu, Haoran and Kim, Young Jin and Atkinson, Adam and Zhan, Zheng and Sun, Jiankai and Peng, Baolin and Liu, Liyuan and Wang, Shuohang and Cheng, Hao and Gao, Jianfeng and Chen, Weizhu and Shen, Yelong},</span>
  <span class="c">year = {2025},</span>
  <span class="c">month = dec,</span>
  <span class="c">journal = {Proceedings of the 39th Conference on Neural Information Processing Systems,},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="ren2025samba" class="col-sm-8"> <div class="title">Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</div> <div class="author"> <em>Liliang Ren</em>,¬†Yang Liu,¬†Yadong Lu,¬†Yelong Shen,¬†Chen Liang,¬†and¬†Weizhu Chen</div> <div class="periodical"> <em>Proceedings of The Thirteenth International Conference on Learning Representations,</em> Apr 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/microsoft/Samba" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1bfK6aSH4O7G_ndIH835KfdQMCcqw8txJ/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> <a href="https://docs.google.com/presentation/d/1SUGKM_sTKgb-wXSYvalWtD2O2qTKnYuMzid669GfJbU/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://arxiv.org/abs/2406.07522" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present SAMBA, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). SAMBA selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall recent memories with the attention mechanism. We scale SAMBA up to 3.8B parameters with 3.2T training tokens and demonstrate that it significantly outperforms state-of-the-art models across a variety of benchmarks. Pretrained on sequences of 4K length, SAMBA shows improved perplexity in context lengths of up to 1M in zero-shot. When finetuned on 4K-length sequences, SAMBA efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits superior retrieval extrapolation on the challenging Phonebook task compared to full-attention models. As a linear-time sequence model, SAMBA achieves a 3.73√ó higher throughput compared to Transformers with grouped-query attention for user prompts of 128K length, and a 3.64√ó speedup when generating 64K tokens with unlimited streaming. Our code for training on open source data is publicly available at https://github.com/microsoft/Samba.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren2025samba</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of The Thirteenth International Conference on Learning Representations,}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="ren2023sparse" class="col-sm-8"> <div class="title">Sparse Modular Activation for Efficient Sequence Modeling</div> <div class="author"> <em>Liliang Ren</em>,¬†Yang Liu,¬†Shuohang Wang,¬†Yichong Xu,¬†Chenguang Zhu,¬†and¬†ChengXiang Zhai</div> <div class="periodical"> <em>Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems,</em> Dec 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/renll/SeqBoat" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1GIrmReqLkc7AyegKXV-C4ns0jy2b-6xd/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> <a href="https://docs.google.com/presentation/d/1a-P_aWkjEDHPWW65ODy5nVrDfiqRjTYi_OgwhAwJO3o/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://arxiv.org/abs/2306.11197" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p> Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren2023sparse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sparse Modular Activation for Efficient Sequence Modeling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Liliang and Liu, Yang and Wang, Shuohang and Xu, Yichong and Zhu, Chenguang and Zhai, ChengXiang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems,}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL DialDoc</abbr></div> <div id="ren2023cpmi" class="col-sm-8"> <div class="title">C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation</div> <div class="author"> <em>Liliang Ren</em>,¬†Mankeerat Sidhu,¬†Qi Zeng,¬†Revanth Gangi Reddy,¬†Heng Ji,¬†and¬†ChengXiang Zhai</div> <div class="periodical"> <em>Proceedings of the ACL2023 Workshop on Document-grounded Dialogue and Conversational Question Answering,</em> Jul 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/renll/C-PMI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1ArAFG2MU0ek5bIqYiWUiJ4x3msieo04g/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://arxiv.org/abs/2306.15245" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Existing reference-free turn-level evaluation metrics for chatbots inadequately capture the interaction between the user and the system. Consequently, they often correlate poorly with human evaluations. To address this issue, we propose a novel model-agnostic approach that leverages Conditional Pointwise Mutual Information (C-PMI) to measure the turn-level interaction between the system and the user based on a given evaluation dimension. Experimental results on the widely used FED dialogue evaluation dataset demonstrate that our approach significantly improves the correlation with human judgment compared with existing evaluation systems. By replacing the negative loglikelihood-based scorer with our proposed CPMI scorer, we achieve a relative 60.5% higher Spearman correlation on average for the FED evaluation metric. Our code is publicly available at https://github.com/renll/C-PMI.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren2023cpmi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Liliang and Sidhu, Mankeerat and Zeng, Qi and Reddy, Revanth Gangi and Ji, Heng and Zhai, ChengXiang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the ACL2023 Workshop on Document-grounded Dialogue and Conversational Question Answering,}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP</abbr><br> <span class="award badge">Oral</span> </div> <div id="ren2022" class="col-sm-8"> <div class="title">Language Model Pre-Training with Sparse Latent Typing</div> <div class="author"> <em>Liliang Ren*</em>,¬†Zixuan Zhang*,¬†Han Wang,¬†Clare Voss,¬†Chengxiang Zhai,¬†and¬†Heng Ji</div> <div class="periodical"> <em>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,</em> Dec 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/renll/SparseLT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1gTMifRSAyj45izkTPLQE5TMsgH-_WSo5/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://arxiv.org/abs/2210.12582" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In this paper, we manage to push the language models to obtain a deeper understanding of sentences by proposing a new pre-training objective, Sparse Latent Typing, which enables the model to sparsely extract sentence-level keywords with diverse latent types. Experimental results show that our model is able to learn interpretable latent type categories in a self-supervised manner without using any external knowledge. Besides, the language model pre-trained with such an objective also significantly improves Information Extraction related downstream tasks in both supervised and few-shot settings. Our code is publicly available at: https://github.com/renll/SparseLT.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Language Model Pre-Training with Sparse Latent Typing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren*, Liliang and Zhang*, Zixuan and Wang, Han and Voss, Clare and Zhai, Chengxiang and Ji, Heng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="ren-etal-2021-hyspa" class="col-sm-8"> <div class="title">HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction</div> <div class="author"> <em>Liliang Ren</em>,¬†Chenkai Sun,¬†Heng Ji,¬†and¬†Julia Hockenmaier</div> <div class="periodical"> <em>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, </em> Aug 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/renll/HySPA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1L-zLdTM5EQqDAtgyjrGQWlrDx9qxnTWz/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> <a href="https://drive.google.com/file/d/1UjIpHx5rMXKgU5O65iHJoX0Od-LmGUcL/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://aclanthology.org/2021.findings-acl.356" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their second-order space/time complexities with respect to the input length. In this work, we propose a Hybrid Span Generator (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-of-the-art on the joint entity and relation extraction task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren-etal-2021-hyspa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{H}y{SPA}: Hybrid Span Generation for Scalable Text-to-Graph Extraction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Liliang and Sun, Chenkai and Ji, Heng and Hockenmaier, Julia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, }</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.findings-acl.356}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.findings-acl.356}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4066--4078}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="ren-etal-2019-scalable" class="col-sm-8"> <div class="title">Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation</div> <div class="author"> <em>Liliang Ren</em>,¬†Jianmo Ni,¬†and¬†Julian McAuley</div> <div class="periodical"> <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), </em> Nov 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/renll/ComerNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1cqiW7QtX_EvgzIlh9DhZnNTwBwPoQ4LS/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> <a href="https://drive.google.com/file/d/1p3SQN_xPNsUGqS4x1XHJ0ySigbG1dcsL/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://aclanthology.org/D19-1196" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Existing approaches to dialogue state tracking rely on pre-defined ontologies consisting of a set of all possible slot types and values. Though such approaches exhibit promising performance on single-domain benchmarks, they suffer from computational complexity that increases proportionally to the number of pre-defined slots that need tracking. This issue becomes more severe when it comes to multi-domain dialogues which include larger numbers of slots. In this paper, we investigate how to approach DST using a generation framework without the pre-defined ontology list. Given each turn of user utterance and system response, we directly generate a sequence of belief states by applying a hierarchical encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre-defined slots. Experiments on both the multi-domain and the single domain dialogue state tracking dataset show that our model not only scales easily with the increasing number of pre-defined domains and slots but also reaches the state-of-the-art performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren-etal-2019-scalable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Liliang and Ni, Jianmo and McAuley, Julian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), }</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Hong Kong, China}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/D19-1196}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/D19-1196}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1876--1885}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">EMNLP</abbr><br> <span class="award badge">Oral</span> </div> <div id="ren-etal-2018-towards" class="col-sm-8"> <div class="title">Towards Universal Dialogue State Tracking</div> <div class="author"> <em>Liliang Ren</em>,¬†Kaige Xie,¬†Lu Chen,¬†and¬†Kai Yu</div> <div class="periodical"> <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, </em> Oct 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/renll/StateNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://drive.google.com/file/d/1aUTcBzDA44fOgU40vPspyNuWu2aR5cgV/view" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a> <a href="https://aclanthology.org/D18-1299" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Dialogue state tracker is the core part of a spoken dialogue system. It estimates the beliefs of possible user‚Äôs goals at every dialogue turn. However, for most current approaches, it‚Äôs difficult to scale to large dialogue domains. They have one or more of following limitations: (a) Some models don‚Äôt work in the situation where slot values in ontology changes dynamically; (b) The number of model parameters is proportional to the number of slots; (c) Some models extract features based on hand-crafted lexicons. To tackle these challenges, we propose StateNet, a universal dialogue state tracker. It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries. Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren-etal-2018-towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Universal Dialogue State Tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren, Liliang and Xie, Kaige and Chen, Lu and Yu, Kai}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, }</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Brussels, Belgium}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/D18-1299}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/D18-1299}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2780--2786}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Liliang Ren. Last updated: November 29, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>